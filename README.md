# AI-Powered Video Advertisement Placement System

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![CUDA 11.8+](https://img.shields.io/badge/CUDA-11.8+-green.svg)](https://developer.nvidia.com/cuda-toolkit)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A comprehensive AI-powered system for intelligent advertisement placement in video content using advanced computer vision techniques including depth estimation, object detection, 3D reconstruction, and photorealistic rendering.

## ğŸ—ï¸ Architecture Overview

The system employs a modular architecture with the following key components:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Video Input    â”‚â”€â”€â”€â–¶â”‚ Depth Estimationâ”‚â”€â”€â”€â–¶â”‚ Object Detectionâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3D Rendering    â”‚â—€â”€â”€â”€â”‚Scene Understandingâ”‚â—€â”€â”€â”€â”‚Camera Estimationâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Temporal Consist.â”‚â”€â”€â”€â–¶â”‚  Final Output   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Technologies
- **Depth Estimation**: Marigold and Depth Pro models
- **Object Detection**: YOLOv9 with ByteTrack tracking
- **3D Reconstruction**: Plane detection and wall identification
- **Rendering**: PBR (Physically Based Rendering) pipeline
- **Temporal Consistency**: Kalman filtering and stabilization

## ğŸ“ Project Structure

```
video_ad_placement/
â”œâ”€â”€ src/video_ad_placement/          # Main source code
â”‚   â”œâ”€â”€ core/                        # Core modules
â”‚   â”‚   â”œâ”€â”€ depth_estimation.py      # Depth estimation pipeline
â”‚   â”‚   â”œâ”€â”€ object_detection.py      # YOLOv9 + ByteTrack
â”‚   â”‚   â”œâ”€â”€ scene_understanding.py   # Plane detection & analysis
â”‚   â”‚   â”œâ”€â”€ camera_estimation.py     # Camera parameters estimation
â”‚   â”‚   â”œâ”€â”€ rendering_engine.py      # PBR rendering & compositing
â”‚   â”‚   â”œâ”€â”€ temporal_consistency.py  # Temporal stabilization
â”‚   â”‚   â””â”€â”€ video_processor.py       # Main processing pipeline
â”‚   â”œâ”€â”€ utils/                       # Utility functions
â”‚   â”œâ”€â”€ models/                      # Model definitions
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ configs/                         # Hydra configuration files
â”œâ”€â”€ tests/                          # Unit and integration tests
â”œâ”€â”€ docs/                           # Sphinx documentation
â”œâ”€â”€ scripts/                        # Utility scripts
â”œâ”€â”€ docker/                         # Docker containerization
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ setup.py                        # Package setup
â”œâ”€â”€ pyproject.toml                  # Project configuration
â”œâ”€â”€ .pre-commit-config.yaml         # Pre-commit hooks
â”œâ”€â”€ .github/workflows/              # CI/CD pipeline
â””â”€â”€ README.md                       # This file
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.9+
- CUDA 11.8+ compatible GPU
- Docker (optional)
- Git

### Installation

#### Option 1: Local Installation
```bash
# Clone the repository
git clone <repository-url>
cd video_ad_placement

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Install pre-commit hooks
pre-commit install

# Install package in development mode
pip install -e .
```

#### Option 2: Docker Installation
```bash
# Build the Docker image
docker build -f docker/Dockerfile -t video-ad-placement .

# Run with GPU support
docker run --gpus all -v $(pwd):/workspace video-ad-placement
```

## ğŸ¯ Usage Examples

### Basic Video Processing
```python
from video_ad_placement.core.video_processor import VideoProcessor
from video_ad_placement.utils.config import load_config

# Load configuration
config = load_config("configs/default_config.yaml")

# Initialize processor
processor = VideoProcessor(config)

# Process video
result = processor.process_video(
    input_path="input_video.mp4",
    output_path="output_video.mp4",
    ad_assets=["ad1.png", "ad2.jpg"]
)

print(f"Processing completed: {result.success}")
```

### Custom Configuration
```python
import hydra
from omegaconf import DictConfig

@hydra.main(config_path="configs", config_name="config")
def main(cfg: DictConfig) -> None:
    processor = VideoProcessor(cfg)
    # Your processing logic here

if __name__ == "__main__":
    main()
```

### Advanced Pipeline Usage
```python
from video_ad_placement.core import (
    DepthEstimator, ObjectDetector, SceneAnalyzer,
    CameraEstimator, RenderingEngine
)

# Initialize individual components
depth_estimator = DepthEstimator(model_type="marigold")
object_detector = ObjectDetector(model_type="yolov9")
scene_analyzer = SceneAnalyzer()
camera_estimator = CameraEstimator()
renderer = RenderingEngine()

# Process frame by frame
for frame in video_frames:
    depth_map = depth_estimator.estimate(frame)
    objects = object_detector.detect(frame)
    planes = scene_analyzer.detect_planes(depth_map, frame)
    camera_params = camera_estimator.estimate(frame, objects)
    
    rendered_frame = renderer.composite_ads(
        frame, depth_map, objects, planes, camera_params, ad_assets
    )
```

## ğŸ”§ Configuration

The system uses Hydra for configuration management. Main configuration files:

- `configs/config.yaml`: Main configuration
- `configs/models/`: Model-specific configs
- `configs/processing/`: Processing pipeline configs
- `configs/rendering/`: Rendering engine configs

Example configuration:
```yaml
# config.yaml
defaults:
  - models: default
  - processing: standard
  - rendering: pbr

video:
  input_format: mp4
  output_format: mp4
  fps: 30

models:
  depth_estimation:
    type: marigold
    checkpoint_path: checkpoints/marigold.pth
  
  object_detection:
    type: yolov9
    confidence_threshold: 0.5
```

## ğŸ§ª Testing

Run the test suite:
```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src/video_ad_placement

# Run specific test categories
pytest tests/unit/
pytest tests/integration/
```

## ğŸ“š Documentation

Build and view documentation:
```bash
cd docs
make html
open _build/html/index.html
```

## ğŸ³ Docker Support

### GPU-Enabled Container
```bash
# Build image with GPU support
docker build -f docker/Dockerfile.gpu -t video-ad-placement:gpu .

# Run with GPU
docker run --gpus all -v $(pwd):/workspace video-ad-placement:gpu
```

### Development Container
```bash
# Build development image
docker build -f docker/Dockerfile.dev -t video-ad-placement:dev .

# Run development container
docker run -it -v $(pwd):/workspace video-ad-placement:dev bash
```

## ğŸ”„ CI/CD Pipeline

The project includes GitHub Actions workflows for:
- Code quality checks (linting, formatting)
- Unit and integration testing
- Documentation building
- Docker image building and publishing

## ğŸ› ï¸ Development

### Code Quality
The project enforces code quality through:
- **Black**: Code formatting
- **isort**: Import sorting
- **flake8**: Linting
- **mypy**: Type checking
- **pre-commit**: Automated checks

### Contributing
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests and quality checks
5. Submit a pull request

## ğŸ“ˆ Performance Benchmarks

| Component | GPU Memory | Processing Time (1080p) |
|-----------|------------|-------------------------|
| Depth Estimation | 4GB | 0.5s/frame |
| Object Detection | 2GB | 0.1s/frame |
| 3D Reconstruction | 1GB | 0.3s/frame |
| Rendering | 3GB | 0.8s/frame |

## ğŸ” Troubleshooting

### Common Issues

1. **CUDA out of memory**
   - Reduce batch size in config
   - Use model quantization
   - Process smaller chunks

2. **Model loading errors**
   - Check checkpoint paths
   - Verify CUDA compatibility
   - Update model versions

3. **Video codec issues**
   - Install ffmpeg properly
   - Check input format support
   - Use compatible codecs

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- Marigold depth estimation team
- YOLOv9 authors
- ByteTrack developers
- Open3D community
- Hydra configuration framework

## ğŸ“ Support

For questions, issues, or contributions:
- Open an issue on GitHub
- Check the documentation
- Review existing discussions

---

Built with â¤ï¸ for intelligent video advertisement placement 